{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems - Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems can be loosely broken down into three categories: content based systems, collaborative filtering systems, and hybrid systems (which use a combination of the other two).\n",
    "![Text](https://cdn-images-1.medium.com/max/1200/1*aSq9viZGEYiWwL9uJ3Recw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content based approach** utilizes a series of discrete characteristics of an item in order to recommend additional items with similar properties.\n",
    " \n",
    "**Collaborative filtering** approach builds a model from a user’s past behaviors (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in.\n",
    " \n",
    "**Hybrid approach** combines the previous two approaches. Most businesses probably use hybrid approach in their production recommender systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content based filtering vs Collaborative filtering\n",
    "**Content based** recommenders analyze the nature of each item. For instance, recommending poets to a user by performing Natural Language Processing on the content of each poet. <br>**Collaborative Filtering**, on the other hand, does not require any information about the items or the users themselves. It recommends items based on users’ past behavior. I will elaborate more on Collaborative Filtering in the following paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboratie Filtering\n",
    "Collaborative filtering leverages the power of the crowd. It uses the actions of users to recommend other items. In general, they can either be **user-based** or **item-based**. Item based approach is usually preferred over user-based approach. User-based approach is often harder to scale because of the dynamic nature of users, whereas items usually don’t change much, and item based approach often can be computed offline and served without constantly re-training.\n",
    "\n",
    "### User-Based (User-User)\n",
    "This algorithm first finds the similarity score between users. Based on this similarity score, it then picks out the most similar users and recommends products which these similar users have liked or bought previously.\n",
    "Here we find look alike customers (based on similarity) and offer products which first customer’s look alike has chosen in past. This algorithm is very effective but takes a lot of time and resources. It requires to compute every customer pair information which takes time. Therefore, for big base platforms, this algorithm is hard to implement without a very strong parallelizable system.\n",
    "\n",
    "![image](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/0o0zVW2O6Rv-LI5Mu1.png)\n",
    "\n",
    "### Item-based (Item-Item)\n",
    "In this algorithm, we compute the similarity score between each pair of items. It measures the similarity between the items that target users rates/ interacts with, and other items.\n",
    "It is quite similar to user based algorithm, but instead of finding customer look alike, we try finding item look alike. Once we have item look alike matrix, we can easily recommend alike items to customer who have purchased any item from the store. This algorithm is far less resource consuming than user-user collaborative filtering. Hence, for a new customer the algorithm takes far lesser time than user-user collaborate as we don’t need all similarity scores between customers. And with fixed number of products, product-product look alike matrix is fixed over time.\n",
    "\n",
    "![image](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/1skK2fqWiBF7weHU8SjuCzw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "Assume there are thre matrices: **M Users**, **N items**, and **H history**. We then use a matrix with size **MxN** to denote the past behavior of users. Each cell in the matrix represents the associated interaction that a user holds. For instance, M_{i, j} denotes how user i likes item j. Such matrix is called **U utility matrix**. Collaborive Filtering (CF) is like filling the blank (cell) in the utility matrix that a user has not seen/rated before based on the similarity between users or items. \n",
    "\n",
    "Example ([movieLens dataset](https://grouplens.org/datasets/movielens/)):\n",
    "* Users: 943 users in the dataset and each user has 5 features, i.e. user_ID, age, sex, occupation and zip_code.<br>\n",
    "![image](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Screenshot-from-2018-05-29-20-28-10.png)\n",
    "\n",
    "* Items: attributes of 1682 movies. There are 24 columns out of which last 19 columns specify the genre of a particular movie. These are binary columns, i.e., a value of 1 denotes that the movie belongs to that genre, and 0 otherwise.<br>\n",
    "![image](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Screenshot-from-2018-05-29-20-30-53.png)\n",
    "\n",
    "* Ratings: 100k ratings for different user and movie combinations.<br>\n",
    "![image](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Screenshot-from-2018-05-29-20-29-52.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - KNN Item-Based Collaborative Filtering with cosine similarity\n",
    "\n",
    "To implement an item based collaborative filtering, KNN is a perfect go-to model and also a very good baseline for recommender system development. But what is the KNN? KNN is a non-parametric, lazy learning method. It uses a database in which the data points are separated into several clusters to make inference for new samples.\n",
    "\n",
    "KNN does not make any assumptions on the underlying data distribution but it relies on item feature similarity. When KNN makes inference about a item, KNN will calculate the “distance” between the target item and every other item in its database, then it ranks its distances and returns the top K nearest neighbor items as the most similar item recommendations.\n",
    "\n",
    "In a real world setting, the vast majority of items receive very few or even no ratings at all by users. Normally we are looking at an extremely sparse matrix with more than 99% of entries are missing values.\n",
    "\n",
    "There are three limitations in the KNN approach. First is the “popularity bias”, the second one is “item cold-start problem”, The last one is the “scalability issue”, if the underlying training data is too big to fit in one machine.\n",
    "\n",
    "- **popularity bias**: refers to system recommends the movies with the most interactions without any personalization\n",
    "- **item cold-start problem**: refers to when items added to the catalogue have either none or very little interactions while recommender rely on the item’s interactions to make recommendations\n",
    "- **scalability issue**: refers to lack of the ability to scale to much larger sets of data when more and more users and items added into our database\n",
    "\n",
    "All three above are very typical challenges for collaborative filtering recommender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Single Value Decomposition (SVD) or Matrix Factorization\n",
    "\n",
    "In collaborative filtering, matrix factorization is the state-of-the-art solution for sparse data problem, although it has become widely known since [Netflix Prize Challenge.](https://www.netflixprize.com/)\n",
    "\n",
    "One way to handle the scalability and sparsity issue created by CF is to leverage a latent factor model to capture the similarity between users and items. Essentially, we want to turn the recommendation problem into an optimization problem. We can view it as: how good we are in predicting the rating for items given a user?.\n",
    "\n",
    "Matrix Factorization is simply a mathematical tool for playing around with matrices. The Matrix Factorization techniques are usually more effective, because they allow users to discover the latent (hidden)features underlying the interactions between users and items.\n",
    "\n",
    "We use singular value decomposition (SVD) — one of the Matrix Factorization models for identifying latent factors.\n",
    "Singular Value Decomposition (SVD) is adopted as shown in the below formula.\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/2000/1*haUDjEiQmG0RapR0SHos6Q.png)\n",
    "\n",
    "X denotes the utility matrix, and U is a left singular matrix, representing the relationship between users and latent factors. S is a diagonal matrix describing the strength of each latent factor, while V transpose is a right singular matrix, indicating the similarity between items and latent factors. \n",
    "\n",
    "Now, you might wonder **what do I mean by latent factor here?** It is a broad idea which describes a property or concept that a user or an item have. For instance, for music, latent factor can refer to the genre that the music belongs to. SVD decreases the dimension of the utility matrix by extracting its latent factors. Essentially, we map each user and each item into a latent space with dimension r. Therefore, it helps us better understand the relationship between users and items as they become directly comparable. \n",
    "\n",
    "The below figure illustrates this idea.\n",
    "![image](https://cdn-images-1.medium.com/max/1600/1*GUw90kG2ltTd2k_iv3Vo0Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Alternating Least Square (ALS) with Spark ML (Scalable Matrix Factorization)\n",
    "\n",
    "Although SVD is very effective in matrix factorization with single machine, it’s not scalable as the amount of data grows today. With terabytes or even petabytes of data, it’s impossible to load data with such size into a single machine. So we need a machine learning model (or framework) that can train on dataset spreading across from cluster of machines.\n",
    "\n",
    "Alternating Least Square (ALS) is also a matrix factorization algorithm and it runs itself in a parallel fashion. ALS is implemented in Apache Spark ML and built for a larges-scale collaborative filtering problems. ALS is doing a pretty good job at solving scalability and sparseness of the Ratings data, and it’s simple and scales well to very large datasets.\n",
    "\n",
    "Documentation [here](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html)\n",
    "\n",
    "Some high-level ideas behind ALS are:\n",
    "\n",
    "* Its objective function is slightly different than SVD: ALS uses L2 regularization while SVD uses L1 regularization\n",
    "* Its training routine is different: ALS minimizes two loss functions alternatively; It first holds user matrix fixed and runs gradient descent with item matrix; then it holds item matrix fixed and runs gradient descent with user matrix\n",
    "* Its scalability: ALS runs its gradient descent in parallel across multiple partitions of the underlying training data from a cluster of machines\n",
    "\n",
    "Just like other machine learning algorithms, ALS has its own set of hyper-parameters. We probably want to tune its hyper-parameters via hold-out validation or cross-validation.\n",
    "\n",
    "Most important hyper-params in Alternating Least Square (ALS):\n",
    "\n",
    "* **maxIter**: the maximum number of iterations to run (defaults to 10)\n",
    "* **rank**: the number of latent factors in the model (defaults to 10)\n",
    "* **regParam**: the regularization parameter in ALS (defaults to 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Smart Adaptive Recommendations (SAR) Algorithm\n",
    "[SAR](https://github.com/Microsoft/Product-Recommendations/blob/master/doc/sar.md) is a practical, rating-free collaborative filtering algorithm for recommendations. Is a fast scalable adaptive algorithm for personalized recommendations based on user transactions history and items description. It produces easily explainable / interpretable recommendations and handles \"cold item\" and \"semi-cold user\" scenarios.\n",
    "\n",
    "The overall architecture of SAR is shown in the following diagram:\n",
    "\n",
    "![image](https://raw.githubusercontent.com/Microsoft/Product-Recommendations/master/images/sar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics for recommendation engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metrics\n",
    "### Recall:\n",
    "* What proportion of items that a user likes were actually recommended\n",
    "* It is given by:\n",
    "![image](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Screenshot-from-2018-05-31-18-20-18-300x85.png)\n",
    "\n",
    "  - Here tp represents the number of items recommended to a user that he/she likes and tp+fn represents the total items that a user likes\n",
    "  - If a user likes 5 items and the recommendation engine decided to show 3 of them, then the recall will be 0.6\n",
    "  - Larger the recall, better are the recommendations\n",
    "  \n",
    "### Precision:\n",
    "* Out of all the recommended items, how many did the user actually like?\n",
    "* It is given by:\n",
    "![image](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Screenshot-from-2018-05-31-18-22-00.png)\n",
    "\n",
    "  - Here tp represents the number of items recommended to a user that he/she likes and tp+fp represents the total items recommended to a user\n",
    "  - If 5 items were recommended to the user out of which he liked 4, then precision will be 0.8\n",
    "  - But consider this case: If we simply recommend all the items, they will definitely cover the items which the user likes. So we have 100% recall! But think about precision for a second. If we recommend say 1000 items and user likes only 10 of them, then precision is 0.1%. This is really low. So, our aim should be to maximize both precision and recall.\n",
    "\n",
    "### RMSE:\n",
    "* It measures the error in the predicted ratings:\n",
    "* It is given by:\n",
    "![image](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/rmse1.png)\n",
    "\n",
    "  - Here, Predicted is the rating predicted by the model and Actual is the original rating\n",
    "  - If a user has given a rating of 5 to a movie and we predicted the rating as 4, then RMSE is 1\n",
    "  - Lesser the RMSE value, better the recommendations\n",
    "\n",
    "\n",
    "## Ranking Metrics\n",
    "\n",
    "The above metrics tell us how accurate our recommendations are but they do not focus on the order of recommendations, i.e. they do not focus on which product to recommend first and what follows after that. We need some metric that also considers the order of the products recommended. So, let’s look at some of the ranking metrics:\n",
    "\n",
    "### Mean Reciprocal Rank:\n",
    "* Evaluates the list of recommendations\n",
    "* It is given by:\n",
    "![image](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Screenshot-from-2018-05-31-18-37-20.png)\n",
    "\n",
    "  - Suppose we have recommended 3 movies to a user, say A, B, C in the given order, but the user only liked movie C. As the rank of movie C is 3, the reciprocal rank will be 1/3\n",
    "  - Larger the mean reciprocal rank, better the recommendations\n",
    "\n",
    "\n",
    "### MAP at k (Mean Average Precision at cutoff k):\n",
    "* Precision and Recall don’t care about ordering in the recommendations\n",
    "* Precision at cutoff k is the precision calculated by considering only the subset of your recommendations from rank 1 through k\n",
    "* It is given by:\n",
    "![image](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/MAp.jpeg)\n",
    "\n",
    "  - Suppose we have made three recommendations [0, 1, 1]. Here 0 means the recommendation is not correct while 1 means that the recommendation is correct. Then the precision at k will be [0, 1/2, 2/3], and the average precision will be (1/3)*(0+1/2+2/3) = 0.38\n",
    "  - Larger the mean average precision, more correct will be the recommendations\n",
    " \n",
    " \n",
    "### NDCG (Normalized Discounted Cumulative Gain):\n",
    "* The main difference between MAP and NDCG is that MAP assumes that an item is either of interest (or not), while NDCG gives the relevance score\n",
    "* It is given by:\n",
    "[Link](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)\n",
    "\n",
    "  - Higher the NDCG value, better the recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABS\n",
    "\n",
    "go here: https://github.com/Microsoft/Recommenders\n",
    "\n",
    "And clone the repo.\n",
    "\n",
    "We are going to be running:\n",
    "\n",
    "- https://github.com/Microsoft/Recommenders/blob/master/notebooks/02_model/baseline_deep_dive.ipynb\n",
    "- https://github.com/Microsoft/Recommenders/blob/master/notebooks/01_prepare_data/data_transform.ipynb\n",
    "- https://github.com/Microsoft/Recommenders/blob/master/notebooks/02_model/sar_deep_dive.ipynb\n",
    "- https://github.com/Microsoft/Recommenders/blob/master/notebooks/02_model/surprise_svd_deep_dive.ipynb\n",
    "- https://github.com/Microsoft/Recommenders/blob/master/notebooks/00_quick_start/sar_movielens_with_azureml.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Datasets\n",
    "- http://jmcauley.ucsd.edu/data/amazon/\n",
    "- http://www2.informatik.uni-freiburg.de/~cziegler/BX/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "name": "Theory",
  "notebookId": 2737456431442507
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
